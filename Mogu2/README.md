# ğŸ•·ï¸ Scraper Universel Fandom

Scraper gÃ©nÃ©rique et intelligent capable de s'adapter automatiquement aux diffÃ©rentes structures HTML des wikis Fandom pour extraire les informations des personnages.

## ğŸ¯ FonctionnalitÃ©s

- **Scraping universel** : S'adapte automatiquement aux diffÃ©rentes structures de wikis Fandom
- **Navigation intelligente** : Trouve automatiquement les catÃ©gories de personnages 
- **Extraction complÃ¨te** : RÃ©cupÃ¨re nom, image, description, type et attributs supplÃ©mentaires
- **Gestion d'erreurs** : GÃ¨re les pages manquantes, redirections, images inaccessibles
- **Rapports dÃ©taillÃ©s** : GÃ©nÃ¨re des logs et statistiques complÃ¨tes
- **Organisation des rÃ©sultats** : Structure automatique des dossiers par fandom

## ğŸ“‹ DonnÃ©es extraites

Pour chaque personnage :
- âœ… **Nom** (obligatoire)
- âœ… **Image principale** (URL, obligatoire) 
- âœ… **Description/Biographie**
- âœ… **Type/RÃ´le/Classe/Origine**
- âœ… **2 attributs supplÃ©mentaires** (pouvoirs, affiliation, etc.)

## ğŸš€ Installation

```bash
# Cloner le projet
cd Mogu2

# Installer les dÃ©pendances
pip install scrapy

# Optionnel: autres dÃ©pendances utiles
pip install requests beautifulsoup4
```

## ğŸ“– Utilisation

### MÃ©thode 1: Script de lancement (RecommandÃ©)

```bash
# Scraper Star Wars
python run_scraper.py https://starwars.fandom.com/wiki/Main_Page

# Scraper Pokemon
python run_scraper.py https://pokemon.fandom.com/wiki/Pokemon_Wiki

# Scraper Marvel
python run_scraper.py https://marvel.fandom.com/wiki/Marvel_Database

# Avec options
python run_scraper.py https://naruto.fandom.com/wiki/Narutopedia --log-level DEBUG --delay 3.0
```

### MÃ©thode 2: Commande Scrapy directe

```bash
# Depuis le dossier Mogu2/
scrapy crawl fandom_spider -a start_url=https://starwars.fandom.com/wiki/Main_Page
```

## ğŸ“ Structure des rÃ©sultats

```
ScrapMogu/
â”œâ”€â”€ result/
â”‚   â””â”€â”€ [nom_fandom]/
â”‚       â””â”€â”€ [nom_fandom]_characters_20240101_120000.json
â””â”€â”€ report/
    â””â”€â”€ [nom_fandom]/
        â””â”€â”€ rapport_[nom_fandom]_20240101_120000.json
```

### Format du fichier JSON de rÃ©sultats

```json
{
  "fandom_name": "starwars",
  "scraped_at": "2024-01-01T12:00:00",
  "total_characters": 150,
  "characters": [
    {
      "name": "Luke Skywalker",
      "image_url": "https://static.wikia.nocookie.net/starwars/images/...",
      "description": "Luke Skywalker Ã©tait un Jedi...",
      "character_type": "Jedi",
      "attribute1_name": "Affiliation", 
      "attribute1_value": "Alliance Rebelle",
      "attribute2_name": "Arme",
      "attribute2_value": "Sabre laser",
      "source_url": "https://starwars.fandom.com/wiki/Luke_Skywalker",
      "fandom_name": "starwars",
      "scraped_at": "2024-01-01T12:00:00"
    }
  ]
}
```

### Format du rapport

```json
{
  "pages_traitees": 200,
  "personnages_trouves": 150,
  "erreurs": [
    "Erreur lors du parsing de https://...: timeout"
  ],
  "pages_ignorees": [
    "https://starwars.fandom.com/wiki/PageSansImage"
  ],
  "start_time": "2024-01-01T12:00:00",
  "end_time": "2024-01-01T12:30:00", 
  "duree_totale": "0:30:00"
}
```

## ğŸ”§ Configuration

Modifiez `Mogu2/settings.py` pour ajuster :

```python
# DÃ©lai entre requÃªtes (respecter les serveurs)
DOWNLOAD_DELAY = 2

# Nombre de tentatives en cas d'erreur
RETRY_TIMES = 3

# User agent
USER_AGENT = "Mogu2 Fandom Scraper (+https://github.com/...)"
```

## ğŸ¤– Fonctionnement

Le scraper suit ce processus intelligent :

1. **Analyse de la page d'accueil** : Recherche les liens vers les catÃ©gories de personnages
2. **Navigation automatique** : Suit les liens de type `/wiki/Category:Characters`
3. **Exploration rÃ©cursive** : GÃ¨re les sous-catÃ©gories automatiquement
4. **Extraction universelle** : S'adapte aux diffÃ©rentes structures HTML
5. **Validation** : VÃ©rifie la prÃ©sence obligatoire du nom et de l'image
6. **Sauvegarde organisÃ©e** : Classe les rÃ©sultats par fandom avec timestamps

## ğŸ› Gestion d'erreurs

Le scraper gÃ¨re automatiquement :
- Pages inaccessibles ou supprimÃ©es
- Images manquantes ou inaccessibles  
- Structures HTML variables
- Timeouts et erreurs rÃ©seau
- Redirections
- Pages vides ou malformÃ©es

## ğŸ“Š Exemples de fandoms testÃ©s

- â­ Star Wars : `https://starwars.fandom.com/wiki/Main_Page`
- ğŸ”¥ Pokemon : `https://pokemon.fandom.com/wiki/Pokemon_Wiki`
- ğŸ¦¸ Marvel : `https://marvel.fandom.com/wiki/Marvel_Database`
- ğŸ´â€â˜ ï¸ One Piece : `https://onepiece.fandom.com/wiki/One_Piece_Wiki`
- ğŸ§™ Harry Potter : `https://harrypotter.fandom.com/wiki/Main_Page`

## âš ï¸ Limites et considÃ©rations

- **Respect des serveurs** : DÃ©lais configurÃ©s pour ne pas surcharger Fandom
- **Robots.txt** : Respecte automatiquement les rÃ¨gles robots.txt
- **Structures variables** : Certains fandoms peuvent nÃ©cessiter des ajustements
- **Images** : Seules les URLs d'images valides sont conservÃ©es
- **Taille** : OptimisÃ© pour des fandoms de taille moyenne (< 1000 personnages)

## ğŸ› ï¸ Personnalisation

Pour adapter le scraper Ã  des structures HTML spÃ©cifiques, modifiez les mÃ©thodes dans `fandom_spider.py` :

- `extract_character_name()` : SÃ©lecteurs pour le nom
- `extract_character_image()` : SÃ©lecteurs pour l'image  
- `extract_character_description()` : SÃ©lecteurs pour la description
- `extract_character_type()` : SÃ©lecteurs pour le type
- `extract_additional_attributes()` : SÃ©lecteurs pour les attributs

## ğŸš§ TODO

- [ ] Support des fandoms multi-langues
- [ ] Cache intelligent des images
- [ ] Interface web pour lancer les scraping
- [ ] Base de donnÃ©es pour stocker les rÃ©sultats
- [ ] API REST pour interroger les donnÃ©es

## ğŸ“ License

MIT License - Voir LICENSE pour plus de dÃ©tails.